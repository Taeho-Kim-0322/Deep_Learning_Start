{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_3_1_RNN_감성분석_데이터전처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taeho-Kim-0322/Deep_Learning_Start/blob/master/%EC%8B%A4%EC%8A%B5_3_1_RNN_%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "# 실습 3. RNN을 이용한 😀감정분석😑 모델 학습하기\n",
        "\n",
        "\n",
        "\n",
        "<b>학습 목표:    \n",
        "- NLU 모델링을 위한 데이터 전처리(토크나이징 & 인코딩, 라벨 만들기)를 이해하고 Python을 사용해 코딩한다.\n",
        "</b>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5VEKjCVzQz"
      },
      "source": [
        "## #0. 실습 준비하기\n",
        "먼저 구글 드라이브를 마운트하고, 필요한 라이브러리를 설치하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cg9V5xCV-YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acf86a0-687b-4e09-e13d-ba11d6bc2bc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSGqvUP_B-SN"
      },
      "source": [
        "konlpy에서 사용하는 jpype를 설치하고, Colab에 반영해 주어야 합니다.   \n",
        "아래 코드를 실행한 후 상단의 [런타임] - [런타임 다시 시작]을 클릭해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ695haKKAwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f8452c-8f3a-415f-c14c-d94b7f614be3"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install jpype1==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7-K8wDgPMKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c7a83f2-3f1a-4ed4-8a85-235e471344f1"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQxrMxDHXEqF"
      },
      "source": [
        "깃허브에 있는 감성분석 데이터셋을 다운로드해 읽어오겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbVmEgziXKav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5d7a06-8d31-484b-fb4d-b924c189889c"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 06:27:13--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14628807 (14M) [text/plain]\n",
            "Saving to: ‘ratings_train.txt’\n",
            "\n",
            "ratings_train.txt   100%[===================>]  13.95M  64.7MB/s    in 0.2s    \n",
            "\n",
            "2020-12-01 06:27:13 (64.7 MB/s) - ‘ratings_train.txt’ saved [14628807/14628807]\n",
            "\n",
            "--2020-12-01 06:27:13--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4893335 (4.7M) [text/plain]\n",
            "Saving to: ‘ratings_test.txt’\n",
            "\n",
            "ratings_test.txt    100%[===================>]   4.67M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-12-01 06:27:13 (65.6 MB/s) - ‘ratings_test.txt’ saved [4893335/4893335]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7QYyS8iOnvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b068bf64-4d39-4021-b6ad-3f977b373c94"
      },
      "source": [
        "\"\"\" 데이터 읽어오기 \"\"\"\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "  raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "  raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "  if int(line[2].strip()) == 0:\n",
        "    FULL_TRAIN.append([line[0], line[1], \"부정\"])\n",
        "  elif int(line[2].strip()) == 1:\n",
        "    FULL_TRAIN.append([line[0], line[1], \"긍정\"])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "  if int(line[2].strip()) == 0:\n",
        "    FULL_TEST.append([line[0], line[1], \"부정\"])\n",
        "  elif int(line[2].strip()) == 1:\n",
        "    FULL_TEST.append([line[0], line[1], \"긍정\"])\n",
        "\n",
        "def print_stat(name, data):\n",
        "  print(\"{:<10}: {}개 (긍정 {}, 부정 {})\".format(name, len(data), len([t for t in data if t[2]==\"긍정\"]), len(data)-len([t for t in data if t[2]==\"긍정\"])))\n",
        "\n",
        "print_stat(\"FULL_TRAIN\", FULL_TRAIN)\n",
        "print_stat(\"FULL_TEST\", FULL_TEST)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL_TRAIN: 150000개 (긍정 74827, 부정 75173)\n",
            "FULL_TEST : 50000개 (긍정 25173, 부정 24827)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkZC-6UY_gZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6803570c-0461-4a6a-a104-75568c16f106"
      },
      "source": [
        "# 데이터 예시 : id, 문장, 라벨 순서\n",
        "FULL_TRAIN[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '부정'],\n",
              " ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '긍정'],\n",
              " ['10265843', '너무재밓었다그래서보는것을추천한다', '부정'],\n",
              " ['9045019', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '부정'],\n",
              " ['6483659',\n",
              "  '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
              "  '긍정'],\n",
              " ['5403919', '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.', '부정'],\n",
              " ['7797314', '원작의 긴장감을 제대로 살려내지못했다.', '부정'],\n",
              " ['9443947',\n",
              "  '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네',\n",
              "  '부정'],\n",
              " ['7156791', '액션이 없는데도 재미 있는 몇안되는 영화', '긍정'],\n",
              " ['5912145', '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?', '긍정']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ozOH9zi-pj"
      },
      "source": [
        "이제 데이터를 train / validation / test 셋으로 나누겠습니다.   \n",
        "학습 시간관계상 train 50000건, validation 10000건, test 10000건만 샘플링해서 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riLyUYubXciI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38de1bd7-144f-4ce5-f235-0741f5836622"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "\n",
        "print_stat(\"train\", train)\n",
        "print(\".. ex:\", train[0])\n",
        "print_stat(\"validation\", val)\n",
        "print(\".. ex:\", val[0])\n",
        "print_stat(\"test\", test)\n",
        "print(\".. ex:\", test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train     : 50000개 (긍정 25036, 부정 24964)\n",
            ".. ex: ['272404', '가슴이 따뜻해지는 영화. 요즘 이런 영화 드물죠.', '긍정']\n",
            "validation: 10000개 (긍정 4937, 부정 5063)\n",
            ".. ex: ['571456', '제목보고 놀랬다', '부정']\n",
            "test      : 10000개 (긍정 5020, 부정 4980)\n",
            ".. ex: ['8066629', '이게 뭐냐 도무지...에효', '부정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLpeP2F3TI7p"
      },
      "source": [
        "## 나누어 놓은 train/ validation/ test 데이터 저장하기\n",
        "import json\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_train.json\", \"w\") as f:\n",
        "  f.write(json.dumps(train))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_val.json\", \"w\") as f:\n",
        "  f.write(json.dumps(val))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_test.json\", \"w\") as f:\n",
        "  f.write(json.dumps(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHdvwAFpEcpS"
      },
      "source": [
        "# #1. 토크나이징\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step1.PNG?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiAtd18qhb9t"
      },
      "source": [
        "먼저 Komoran 형태소분석기를 사용해 형태소 분석 함수 tokenize를 정의하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8uFroJgKwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d043461b-5dff-4330-f770-767bc1ecf396"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return komoran.morphs(sentence)\n",
        "\n",
        "tokenize(\"미션 완료!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['미션', '완료', '!!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK1JEphdYwyU"
      },
      "source": [
        "# #2. 단어 사전 로딩 & 인코딩\n",
        "이제 인코딩을 위해 단어사전을 로딩하겠습니다.   \n",
        "실습 소개에서 말씀드렸던 것처럼 CBOW 학습에서 사용한 70002개의 토큰에    \n",
        "이번 태스크 수행에서 새롭게 나온 토큰을 추가하겠습니다.\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step2.PNG?raw=true\">   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-cDT132ciHp"
      },
      "source": [
        "#### Step 1. 단어사전 로딩하기\n",
        "- 기 학습된 단어 임베딩을 불러와 사용\n",
        "- 도메인 특화적인 단어로 [UNK]가 되는 토큰들을 식별해 단어 사전에 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACfKwipLZEze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf2ac29-f4af-40ff-a621-dea30e581aa7"
      },
      "source": [
        "\"\"\" 지난 실슬 때 만들었던 단어 사전 로딩 \"\"\"\n",
        "\n",
        "## final_embeddings: 70002개 토큰에 대한 워드 벡터 매트릭스 shape=(70002, 128)\n",
        "## vocab_list: 위의 워드 벡터와 매칭되는 단어 사전\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/vecs.tsv\") as f:\n",
        "  vecs = [v.strip() for v in f.readlines()]\n",
        "final_embeddings = [v.split(\"\\t\") for v in vecs]\n",
        "final_embeddings = np.array(final_embeddings, dtype=\"float32\")\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/meta.tsv\") as f:\n",
        "  vocab_list = [v.strip() for v in f.readlines()]\n",
        "\n",
        "print(\"** 단어 사전 개수:\", len(vocab_list))\n",
        "print(\"** 단어벡터 예시: \")\n",
        "print(\"token :\", vocab_list[999])\n",
        "print(\"vector:\", final_embeddings[999])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 단어 사전 개수: 70002\n",
            "** 단어벡터 예시: \n",
            "token : 약속\n",
            "vector: [-0.08241888 -0.3996674   0.7739711  -0.16975428  0.3806158   0.45241347\n",
            "  0.05767588 -0.22240417 -0.35989466  0.2757637  -0.03140675 -0.5258993\n",
            "  0.12905656  0.06397405  0.30114022 -0.07181059 -0.02570166 -0.48795968\n",
            " -0.06511765 -0.18512052  0.19694534  0.23895912  0.4805085   0.37195817\n",
            " -0.25638524 -0.05839769 -0.35014316 -0.6704571   0.26206943 -0.04719381\n",
            " -0.2411261  -0.3969594   0.06482407 -0.03239198  0.02542533 -0.41497877\n",
            "  0.20100671 -0.58823097  0.56744164  0.16191098 -0.42996758 -0.22327337\n",
            " -0.12769872 -0.5433165   0.29652247 -0.5026788  -0.22542043  0.0512139\n",
            " -0.47466087 -0.00419373 -0.28738588  0.02315034 -0.5638739   0.05323079\n",
            " -0.50659144  0.26971442 -0.10166231 -0.20791733  0.06511372 -0.06318088\n",
            "  0.9490145   0.2286897  -0.2832847  -0.31193975  0.09339825  0.11727391\n",
            " -0.5285182  -0.30312613 -0.07310445 -0.11501949  0.00234404  0.4665634\n",
            "  0.23767684 -0.02189105  0.38123113 -0.37145177  0.5473988  -0.15776537\n",
            "  0.07874138 -0.2076071   0.30245492 -0.09582748  0.06326489 -0.1065819\n",
            " -0.07606437  0.02219253  0.05043362  0.2524615   0.34576225  0.25786257\n",
            " -0.399961    0.04404035 -0.490413   -0.5224413   0.19621475 -0.04770812\n",
            "  0.01687264 -0.24496874  0.6422873  -0.31506142  0.02313692 -0.03274352\n",
            "  0.11555163 -0.24784558 -0.22561093  0.21938592 -0.23525222 -0.2675352\n",
            "  0.30050626  0.5500072  -0.01259814  0.13869031  0.40448198  0.16987054\n",
            "  0.12881851  0.10437147  0.47539017 -0.58405995  0.12981346 -0.14040871\n",
            " -0.4901492   0.24698702 -0.21243201  0.11441316  0.24703455 -0.06589016\n",
            " -0.55630255  0.19620106]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHzLQxYSlWI"
      },
      "source": [
        "[링크 텍스트](https://)<font color=\"blue\"> 🙋‍♀️[QUIZ] final_embeddings의 차원이 70002 x 128인 이유는 무엇인가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJDxTsjChDKs"
      },
      "source": [
        "현재 태스크에서 [UNK]로 떨어지는 단어가 어떤 것들이 있는지 살펴보겠습니다.   \n",
        "CBOW 실습에서 사용했던 collections의 Counter()기능을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IisJ8WUwZUV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a0e2ca-b678-4a57-b7af-bab6db831b5e"
      },
      "source": [
        "import collections \n",
        "from tqdm import tqdm\n",
        "tot_tokens = 0\n",
        "oov_counter = collections.Counter() #새로운 토큰 카운터\n",
        "\n",
        "Tokenized_train = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(train):\n",
        "  sent = dat[1]\n",
        "  tokenized_sent = tokenize(sent)\n",
        "  tot_tokens += len(tokenized_sent) #토큰 개수\n",
        "  for word in tokenized_sent:\n",
        "    if word not in vocab_list: #기존 단어사전에서 찾을 수 없으면\n",
        "      oov_counter[word] += 1 #Counter에서 개수 세기\n",
        "      \n",
        "  Tokenized_train.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [02:02<00:00, 408.95it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8WfRvinZUh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144bcd9e-47d6-45f1-8fde-753b46de68f9"
      },
      "source": [
        "print(\"# OOV Tokens:\", len(oov_counter))\n",
        "print(\"{}/{} ({:.2f}%) are [UNK] in train tokens\".format(sum(oov_counter.values()) , tot_tokens , 100*sum(oov_counter.values())/tot_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# OOV Tokens: 15837\n",
            "35785/952499 (3.76%) are [UNK] in train tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwbm16L8T3kz"
      },
      "source": [
        "👉 50,000개의 train 문장에서 무려 15,000개 이상의 Out-Of-Vocabulary 토큰들이 발견되었습니다.    \n",
        "🙋‍♀️ 어떤 토큰들이 기존 단어 사전에 포함되어 있지 않았을까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9O8yqJea7E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9ad797-05bf-4250-c00a-58cb459e4926"
      },
      "source": [
        "most_common = oov_counter.most_common(len(oov_counter))\n",
        "print(most_common[:10])\n",
        "print(most_common[-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ㅋㅋ', 616), ('......', 533), ('♥', 523), ('ㅋㅋㅋ', 340), ('ㅋ', 321), ('ㅠㅠ', 307), ('이쁘', 268), ('ㅎㅎ', 252), ('ㅡㅡ', 249), ('막장', 231)]\n",
            "[('훠~~얼씬', 1), ('재밌는데요?ㅋㅋㅋ', 1), ('인고', 1), ('사겼고', 1), ('덱스터', 1), ('됬어요', 1), ('ㅋㅋ어쨌든', 1), ('멕클레리', 1), ('개사기캐릭;;;적수가', 1), ('하나..ㅋ', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXz1rwmUMHH"
      },
      "source": [
        "예상했던 것과 같이 구어체적인 표현들이 새로 등장한 것을 볼 수 있습니다.   \n",
        "\n",
        "\n",
        "이제 새로 나온 토큰들을 기존의 단어사전에 추가해 감성분석 태스크 분석을 위한 새로운 단어사전을 만들겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdRpNRLafFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db88488-ee18-4c71-91f5-b9cc22953ce2"
      },
      "source": [
        "### Train 데이터에서 새로 발견한 토큰을 기존 단어 사전에 추가\n",
        "new_vocab_list = vocab_list.copy() # 1. 기존 단어 사전 복사\n",
        "new_vocab_list.extend([v[0] for v in most_common]) # 2. 새로 나온 토큰을 기존 리스트에 이어붙이기\n",
        "print(\"# New Vocabs = {}\".format(len(new_vocab_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# New Vocabs = 85839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0m667zbUon4"
      },
      "source": [
        "<font color=\"blue\"> 🙋‍♀️[QUIZ] 새로 나온 토큰을 기존 리스트의 뒤에 이어붙여야 하는 이유는 무엇인가요?   \n",
        "🙋‍♀️[QUIZ] 새로운 단어사전은 몇 개의 토큰이 있나요? 모델에서 임베딩 차원은 몇 차원이 되어야 하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbgukIuZU7Lk"
      },
      "source": [
        "새로 만든 단어사전을 저장해 놓아야 이후 학습~추론에서 사용할 수 있겠지요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piDQCOt_0i5Z"
      },
      "source": [
        "## 새로 만든 단어사전 저장\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_vocab.json\", \"w\") as f:\n",
        "  f.write(json.dumps(new_vocab_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZZ2J9kUWqq_"
      },
      "source": [
        "#### Step 2. 데이터 토크나이징하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fGQbvXbVLcM"
      },
      "source": [
        "그럼 validation과 test 데이터에 대해서도 토크나이징을 진행하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWG3pA0p2e0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83124360-27af-4d8a-a04d-9dcdc7e3f6a6"
      },
      "source": [
        "Tokenized_val = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(val):\n",
        "  tokenized_sent = tokenize(dat[1])   \n",
        "  Tokenized_val.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장\n",
        "\n",
        "Tokenized_test = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(test):\n",
        "  tokenized_sent = tokenize(dat[1])   \n",
        "  Tokenized_test.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1445.28it/s]\n",
            "100%|██████████| 10000/10000 [00:06<00:00, 1485.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avaO54uS2e5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f38e79d-25ec-4bcf-ff60-15c15015e319"
      },
      "source": [
        "print(len(Tokenized_train) , len(Tokenized_val), len(Tokenized_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enZmxc4OWhTC"
      },
      "source": [
        "#### Step 3. 텍스트 인코더 코딩하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuO8hmMWHIR"
      },
      "source": [
        "토크나이징된 문장을 인덱스로 바꾸기 위해, 지난 모듈에서 정의했던 TextEncoder를 사용하겠습니다.    \n",
        "지난 시간에 코딩했던 내용을 utils.py에 담아두었습니다.   \n",
        "여기서 TextEncoder를 불러와 사용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZkpgi9Eh5a"
      },
      "source": [
        "from utils import TextEncoder\n",
        "text_encoder = TextEncoder(new_vocab_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1odlXGscJ1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb20ede-d08d-4ab2-e3fb-3091b1c01e61"
      },
      "source": [
        "## 토크나이저 테스트\n",
        "sent = \"새로 만든 단어사전 ♥\"\n",
        "tokenized_sent = tokenize(sent)\n",
        "tokenized_ids = text_encoder.convert_tokens_to_ids(tokenized_sent) # 토큰 -> 인덱스\n",
        "reversed_token= text_encoder.convert_ids_to_tokens(tokenized_ids) # 인덱스 -> 토큰\n",
        "\n",
        "print(tokenized_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1339, 128, 9, 1313, 1815, 70004]\n",
            "['새로', '만들', 'ㄴ', '단어', '사전', '♥']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8dBaSmcqCY"
      },
      "source": [
        "### Step 4. 태스크 수행을 위한 형태로 만들기   \n",
        "그럼 이제 본격적으로 감성분석 태스크를 수행할 수 있는 형태로 인풋을 준비하겠습니다.   \n",
        "\n",
        "이를 위해 아래의 세 가지를 수행해야 합니다.   \n",
        "1. 인풋 자연어 토큰을 정수 인덱스로 변환(@text_encoder.convert_tokens_to_ids)\n",
        "2. 정답 라벨을 정수 인덱스로 변환 -> 라벨 매핑 사전 필요   \n",
        "3. 배치 처리를 위해 패딩 & numpy array로 변환\n",
        "\n",
        "\n",
        "위의 세 가지 작업을 수행하는 함수로 create_cls_feature이라는 함수를 정의하겠습니다.    \n",
        "TextEncoder와 마찬가지로, 이 함수는 다양한 텍스트 분류 과제에서 코드 재활용이 가능합니다. \n",
        "\n",
        "create_cls_feature\n",
        "- 함수 인풋: 데이터 , text_encoder, max_seq_len\n",
        "- 함수 아웃풋: input_ids , labels, label_map   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5UdYcieZ7ef"
      },
      "source": [
        "그럼 데이터 example 일부에 대해 한 단계씩 차례대로 수행하며 차례대로 코딩해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWGM-lpgYblR"
      },
      "source": [
        "<b>1. 인풋 자연어 토큰을 정수 인덱스로 변환(@text_encoder.convert_tokens_to_ids)   </b>   \n",
        "함수는 인풋으로 Tokenized_train과 같은 데이터를 받습니다.   \n",
        "데이터는 리스트의 리스트로 이루어져 있으며, 각각의 리스트는 [문장 번호, 토큰화된 문장, 정답 라벨]의 모양으로 되어 있습니다.   \n",
        "(예) ['7570203', ['장쯔이', '그때', '나', '지금', '이나', '이뻤다'], '긍정']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFeFXFoYwlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59faa9ef-8aec-4fba-de81-50f5899b6567"
      },
      "source": [
        "examples = Tokenized_train[:3]\n",
        "\n",
        "for example in examples:\n",
        "  print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['272404', ['가슴', '이', '따뜻', '하', '아', '지', '는', '영화', '.', '요즘', '이런', '영화', '드물', '죠', '.'], '긍정']\n",
            "['5294639', ['정말', '잘', '~', '만들', 'ㄴ', '영화', '.'], '긍정']\n",
            "['9678288', ['10대', '여자', '들', '이나', '보', '면', '딱', '좋', '을', '영화', '..', '질', '떨어지', '는', '영화', '가', '관객', '수', '가', '머', '가', '이', '이', '라도', '많', '아', '?', '나', '참', '기가', '막히', '어', '.'], '부정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjWcIQxqZPbo"
      },
      "source": [
        "<font color=\"red\"> [MISSION] examples에 있는 토큰화된 자연어 문장들을 인덱스로 변환해 input_ids라는 리스트에 저장해보세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfS-sNNIZO_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0e9c8c-d903-4153-8105-64c464013e98"
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "for example in examples:\n",
        "  idx, tokenized_sent, label = example\n",
        "  input_id = text_encoder.convert_tokens_to_ids(tokenized_sent) ## [★ CODE ★]\n",
        "  input_ids.append(input_id)\n",
        "\n",
        "for i, input_id in enumerate(input_ids):\n",
        "  print(\"문장 {}:\".format(i), input_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 0: [3427, 3, 3940, 2, 18, 34, 6, 162, 4, 7436, 472, 162, 2284, 2857, 4]\n",
            "문장 1: [2933, 359, 183, 128, 9, 162, 4]\n",
            "문장 2: [2364, 505, 23, 150, 85, 74, 7276, 308, 7, 162, 45919, 1756, 653, 6, 162, 20, 1988, 41, 20, 2006, 20, 3, 3, 1517, 102, 18, 867, 75, 2062, 4538, 4700, 26, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4igxqa1aFT8"
      },
      "source": [
        "\n",
        "<b>2. 정답 라벨을 정수 인덱스로 변환 -> 라벨 매핑 사전 필요    </b>   \n",
        "\n",
        "다음으로 모델에게 라벨을 알려줄 수 있는 라벨 매핑 사전이 필요합니다.    \n",
        "모델은 \"긍정\"이나 \"부정\" 이라는 단어를 인식할 수 있기 때문에, 각 라벨에 해당하는 정수 인덱스를 매핑해주는 것입니다.   \n",
        "{\"긍정\": 0 , \"부정\": 1} 이런 식으로요.   \n",
        "\n",
        "이 때 라벨 매핑 사전은 학습~추론에서 유지되어야 하겠지요?   \n",
        "따라서 학습 데이터를 이용해   \n",
        "1. 새로 발견된 라벨을 라벨 매핑 사전에 저장한다.\n",
        "2. 완성된 라벨 매핑 사전을 저장해둔다.   \n",
        "\n",
        "이후 검증/ 테스트 데이터에서는 위에서 만든 라벨 매핑 사전을 사용해 매핑만 해주도록 코드를 구현하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMwuDNZlb673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831daef8-e98f-4b02-c9b4-7c2151238370"
      },
      "source": [
        "CREATE_LABEL_MAP = True # 새로 사전을 만들겠다. \n",
        "label_map = {}\n",
        "label_index = 0 \n",
        "\n",
        "label_ids = [] ## 데이터에 대해 변환된 라벨 저장\n",
        "for example in examples:\n",
        "  idx, tokenized_sent, label = example\n",
        "  if label not in label_map: # 라벨이 label_map에 없다면\n",
        "    if CREATE_LABEL_MAP:\n",
        "      label_map[label] = label_index # 라벨 맵에 해당 라벨 추가\n",
        "      label_index += 1\n",
        "      label_id = label_map[label]      \n",
        "    else:\n",
        "      ## train 이외의 단계에서 새로운 라벨이 발견되었다면, 이는 잘못된 데이터입니다. \n",
        "      ## 따라서 Error 메시지를 추가하고, 건너뛰도록 하겠습니다.\n",
        "      print(\"** ERROR: UNSEEN LABEL DETECTED -\", label)\n",
        "      continue\n",
        "  else: # 라벨을 찾았으면\n",
        "    label_id = label_map[label]\n",
        "  label_ids.append(label_id)\n",
        "\n",
        "print(\"라벨 매핑 사전:\", label_map)\n",
        "print(\"-> 원래 라벨:\", [e[2] for e in examples])\n",
        "print(\"-> 인덱싱 후:\", label_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "라벨 매핑 사전: {'긍정': 0, '부정': 1}\n",
            "-> 원래 라벨: ['긍정', '긍정', '부정']\n",
            "-> 인덱싱 후: [0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7-rRYFKda7k"
      },
      "source": [
        "<b>3. 배치 처리를 위해 패딩 & numpy array로 변환    </b>   \n",
        "\n",
        "정수 인덱스로 변환한 input_ids를 살펴볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B27syNaIdpA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0156daaf-7c18-406c-8935-43794ded17fc"
      },
      "source": [
        "for i, input_id in enumerate(input_ids):\n",
        "  print(\"문장 {}:\".format(i), input_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 0: [3427, 3, 3940, 2, 18, 34, 6, 162, 4, 7436, 472, 162, 2284, 2857, 4]\n",
            "문장 1: [2933, 359, 183, 128, 9, 162, 4]\n",
            "문장 2: [2364, 505, 23, 150, 85, 74, 7276, 308, 7, 162, 45919, 1756, 653, 6, 162, 20, 1988, 41, 20, 2006, 20, 3, 3, 1517, 102, 18, 867, 75, 2062, 4538, 4700, 26, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkE_YPI_dqMG"
      },
      "source": [
        "👉자연어 문장은 길이가 제각각이기 때문에 각 문장의 길이가 제각각인 것을 볼 수 있습니다.   \n",
        "\n",
        "하지만 딥러닝 배치 처리를 위해서는 이들을 일정한 길이로 맞춰주어야 하지요.   \n",
        "이 기능을 수행해 주는 것이 tensorflow.keras.preprocessing.sequence의 pad_sequences 기능입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bARuY8gfYkDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8ef255-6a28-4077-b593-21a2318b6505"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_len = 20\n",
        "\n",
        "input_ids_1 = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"post\", # 패딩은 문장의 뒤에 한다\n",
        "                            truncating=\"post\" ) # 최대 길이를 넘어갈 경우 뒷 부분을 자른다\n",
        "print(\"** 예시 1:\")\n",
        "print(input_ids_1)\n",
        "input_ids_2 = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"pre\", # 패딩은 문장의 앞에 한다\n",
        "                            truncating=\"pre\" ) # # 최대 길이를 넘어갈 경우 앞 부분을 자른다\n",
        "print(\"\\n** 예시 2:\")\n",
        "print(input_ids_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 예시 1:\n",
            "[[ 3427     3  3940     2    18    34     6   162     4  7436   472   162\n",
            "   2284  2857     4     0     0     0     0     0]\n",
            " [ 2933   359   183   128     9   162     4     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [ 2364   505    23   150    85    74  7276   308     7   162 45919  1756\n",
            "    653     6   162    20  1988    41    20  2006]]\n",
            "\n",
            "** 예시 2:\n",
            "[[   0    0    0    0    0 3427    3 3940    2   18   34    6  162    4\n",
            "  7436  472  162 2284 2857    4]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0 2933\n",
            "   359  183  128    9  162    4]\n",
            " [   6  162   20 1988   41   20 2006   20    3    3 1517  102   18  867\n",
            "    75 2062 4538 4700   26    4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUeAbTSPYjdQ"
      },
      "source": [
        "<font color=\"red\"> [MISSION] 한국어에서 핵심적인 정보는 문장의 뒷 부분에 있다고 판단하였습니다.   \n",
        "따라서, 문장이 max_seq_len을 넘어가면 앞부분을 자르고, 모자르면 문장 뒤에 0 패딩을 하기로 결정하였습니다.   \n",
        "pad_sequences 함수를 사용해 이 기능을 수행할 수 있도록 직접 코딩해보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMIs9ehfAKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf4bf9b-4570-48a0-ab51-14ba5be361b9"
      },
      "source": [
        "\"\"\" Your Code Here \"\"\"\n",
        "input_ids = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"post\", # 패딩은 문장의 뒤에 한다\n",
        "                            truncating=\"pre\" ) # # 최대 길이를 넘어갈 경우 앞 부분을 자른다\n",
        "print(\"\\n** 예시 3:\")\n",
        "print(input_ids)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** 예시 3:\n",
            "[[3427    3 3940    2   18   34    6  162    4 7436  472  162 2284 2857\n",
            "     4    0    0    0    0    0]\n",
            " [2933  359  183  128    9  162    4    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]\n",
            " [   6  162   20 1988   41   20 2006   20    3    3 1517  102   18  867\n",
            "    75 2062 4538 4700   26    4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uzYr10RfIYV"
      },
      "source": [
        "그럼 위에서 하나씩 만들었던 함수를 모두 합쳐서 create_cls_feature 함수를 만들겠습니다.   \n",
        "이 함수는 utils.py에도 저장되어 있어, 향후에도 텍스트 분석에서 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upowc1lDca9w"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def create_cls_feature(examples, text_encoder, max_seq_len, label_map=None):\n",
        "\n",
        "  input_ids = [] # 정수 인덱스로 변환한 문장들의 리스트\n",
        "  labels = [] # 정답 라벨 리스트\n",
        "\n",
        "  if label_map is None: #label_map이 없으면 -> 이번에 데이터를 처리하면서 새로 생성함\n",
        "    CREATE_LABEL_MAP = True\n",
        "    label_map = {}\n",
        "    label_index = 0\n",
        "  else: \n",
        "    CREATE_LABEL_MAP = False\n",
        "    print(\"** Start creating features using label map\")\n",
        "    print(label_map)\n",
        "\n",
        "  for example in examples:\n",
        "    idx, tokenized_sent, label = example\n",
        "\n",
        "    ## 1. text_encoder 사용해 정수로 변환 \n",
        "    input_id = text_encoder.convert_tokens_to_ids(tokenized_sent)\n",
        "    if len(input_id) == 0:\n",
        "      print(\"Sentence with length = 0... continue\", example)\n",
        "      continue\n",
        "\n",
        "    ## 2. label 매핑 & index로 변환\n",
        "    if label in label_map:\n",
        "      label_id = label_map[label]\n",
        "    else:\n",
        "      if CREATE_LABEL_MAP:\n",
        "        # label map에 추가\n",
        "        label_map[label] = label_index\n",
        "        label_index += 1\n",
        "        label_id = label_map[label]\n",
        "      else:\n",
        "        print(\"** ERROR: UNSEEN LABEL DETECTED -\", label)\n",
        "        continue\n",
        "  \n",
        "    ## 전체 리스트에 append\n",
        "    input_ids.append(input_id)\n",
        "    labels.append(label_id)\n",
        "    \n",
        "      \n",
        "  \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "  input_ids = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, \n",
        "                            padding=\"post\", \n",
        "                            truncating=\"pre\")\n",
        "\n",
        "  ## np.array로 변환해 리턴\n",
        "  input_ids = np.array(input_ids)\n",
        "  labels = np.array(labels)\n",
        "  \n",
        "  assert len(input_ids) == len(labels)\n",
        "  print(\"** {} examples processed\".format(len(input_ids)))\n",
        "\n",
        "  return input_ids, labels, label_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AchPbrDWl4X_"
      },
      "source": [
        "함수를 사용해 train, validation, test 데이터를 각각 처리하겠습니다.   \n",
        "train 데이터를 만들 때는 label_map을 인풋에 주지 않아, 전처리중 label_map이 생성되도록 합니다.   \n",
        "validation과 test 데이터를 만들 때에는 학습 데이터에 대해 만들어진 label_map을 사용해 전처리를 진행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUuJxeHtnTBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15530959-4dea-42ff-926f-6909948f037d"
      },
      "source": [
        "# TRAIN\n",
        "train_ids, train_labels, label_map = create_cls_feature(Tokenized_train, text_encoder, max_seq_len=50, label_map = None)\n",
        "# VAL\n",
        "val_ids, val_labels, _ = create_cls_feature(Tokenized_val, text_encoder, max_seq_len=50, label_map = label_map)\n",
        "# TEST\n",
        "test_ids, test_labels, _ = create_cls_feature(Tokenized_test, text_encoder, max_seq_len=50, label_map = label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence with length = 0... continue ['1034280', [], '부정']\n",
            "Sentence with length = 0... continue ['1034283', [], '부정']\n",
            "** 49998 examples processed\n",
            "** Start creating features using label map\n",
            "{'긍정': 0, '부정': 1}\n",
            "** 10000 examples processed\n",
            "** Start creating features using label map\n",
            "{'긍정': 0, '부정': 1}\n",
            "** 10000 examples processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FRAzerdca6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ffe9438-e9db-4f36-95c0-9abbbfad955f"
      },
      "source": [
        "print(\"# Train={} # Val={} # Test={}\".format(len(train_ids), len(val_ids), len(test_ids)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Train=49998 # Val=10000 # Test=10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cep29zj6hLjV"
      },
      "source": [
        "모델링을 위한 데이터 전처리가 완료되었습니다!    \n",
        "그럼 데이터를 저장하고, 다음 페이지에서 모델링을 계속 진행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYZdu4fphWcS"
      },
      "source": [
        "import pickle\n",
        "prepro_data = {\n",
        "    \"train_ids\": train_ids,\n",
        "    \"train_labels\": train_labels,\n",
        "    \"val_ids\": val_ids,\n",
        "    \"val_labels\": val_labels,\n",
        "    \"test_ids\": test_ids,\n",
        "    \"test_labels\": test_labels,\n",
        "    \"label_map\":label_map\n",
        "}\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_prepro_data.pkl\", \"wb\") as f:\n",
        "  pickle.dump(prepro_data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxUy68PJiIze"
      },
      "source": [
        "utils.py 파일은 드라이브에 저장해, 향후 활용할 수 있도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO_pv12H9YQZ"
      },
      "source": [
        "!cp utils.py \"/content/gdrive/My Drive/NLP/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dskB09KlCf1I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}